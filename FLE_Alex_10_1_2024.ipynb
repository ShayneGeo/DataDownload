{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dae96c-7a7f-4e9b-b509-2e1f0a56b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Polygon, LineString\n",
    "# import utm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# interval_distance = 90\n",
    "# buff = 350\n",
    "# threshold_distance = 350\n",
    "\n",
    "# # Path to the shapefile\n",
    "# shapefile_path = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\WFIGFirePerimeters2021\\Perimeters.shp\"\n",
    "# #shapefile_path = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\scratch\\WINDY.shp\"\n",
    "# print(\"4\")\n",
    "# # Read the shapefile into a GeoDataFrame\n",
    "# gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# # Filter for the specific perimeter\n",
    "# #specific_perimeter_name = \"{ACD5C207-BB83-4419-977D-3F64778081A3}\"\n",
    "# #gdf = gdf[gdf['poly_IRWIN'] == specific_perimeter_name]\n",
    "\n",
    "# # Filter the GeoDataFrame for polygons with an area greater than 1000 acres\n",
    "# #filtered_gdf = gdf[gdf['poly_Acres'] > 1000]\n",
    "# print(\"5\")\n",
    "\n",
    "# # Sort the filtered GeoDataFrame by the 'poly_Acres' column in descending order\n",
    "# filtered_gdf = gdf.sort_values(by='poly_Acres', ascending=False)\n",
    "# print(\"6\")\n",
    "\n",
    "# # Input folder and POD shapefile path\n",
    "# OutputFolder = \"C:\\\\Users\\\\magst\\\\OneDrive\\\\Pictures\\\\Desktop\\\\FLE_SamplePoints_1\\\\\"\n",
    "# POD_shapefile_path = \"C:\\\\Users\\\\magst\\\\OneDrive\\\\Pictures\\\\Desktop\\\\CONUSDATA\\\\FL_2021.shp\"\n",
    "# print(\"7\")\n",
    "\n",
    "# def generate_points_along_line(line, interval_distance):\n",
    "#     total_length = line.length\n",
    "#     num_points = int(total_length // interval_distance) + 1\n",
    "#     points = [line.interpolate(i * interval_distance) for i in range(num_points)]\n",
    "#     return points\n",
    "\n",
    "# # Read the POD shapefile and filter the required lines\n",
    "# test_pod = gpd.read_file(POD_shapefile_path)\n",
    "# #test_pod = test_pod[test_pod['FirelineCa'].isin([1, 2, 3])]\n",
    "# test_pod = test_pod[test_pod['FirelineCa'].isin([1, 2, 3]) & (test_pod['FirelineEn'] != 'Not Engaged')]\n",
    "\n",
    "# print(\"8\")\n",
    "\n",
    "# for i, perimeter in enumerate(filtered_gdf.itertuples()):\n",
    "#     try:\n",
    "#         fire_perimeters = gpd.GeoDataFrame([perimeter], geometry='geometry')\n",
    "#         print(\"66\")\n",
    "#         # Set CRS if missing\n",
    "#         if fire_perimeters.crs is None:\n",
    "#             fire_perimeters.set_crs(epsg=4326, inplace=True)\n",
    "#         print(\"77\")\n",
    "#         lon, lat = fire_perimeters.geometry.iloc[0].centroid.coords[0]\n",
    "\n",
    "#         centroid_lon = lon\n",
    "#         centroid_lat = lat\n",
    "\n",
    "#         utm_zone = utm.from_latlon(lat, lon)[2]\n",
    "#         utm_crs = f'EPSG:326{utm_zone}'\n",
    "#         fire_perimeters = fire_perimeters.to_crs(utm_crs)\n",
    "#         fire_perimeters['geometry'] = fire_perimeters['geometry'].buffer(0)\n",
    "#         print(\"88\")\n",
    "#         outward_buffer = fire_perimeters.geometry.buffer(buff)\n",
    "#         inward_buffer = fire_perimeters.geometry.buffer(-buff)\n",
    "#         inward_buffer_BREACH = fire_perimeters.geometry.buffer(-buff - 300)\n",
    "#         combined_buffers = outward_buffer.difference(inward_buffer)\n",
    "#         buffered_gdf = gpd.GeoDataFrame(geometry=combined_buffers, crs=fire_perimeters.crs)\n",
    "\n",
    "#         # Transform the test_pod to match the CRS of the buffered_gdf\n",
    "#         test_pod_transformed = test_pod.to_crs(buffered_gdf.crs)\n",
    "#         print(\"99\")\n",
    "#         buffered_fire_perimeters = fire_perimeters.buffer(buff)\n",
    "#         test_pod_clipped = gpd.clip(test_pod_transformed, buffered_fire_perimeters)\n",
    "#         print(\"1\")\n",
    "\n",
    "#         held_pod = gpd.clip(buffered_gdf, test_pod_clipped)\n",
    "#         print(f\"Number of held POD geometries: {len(held_pod)}\")\n",
    "#         if held_pod.empty:\n",
    "#             print(f\"No held POD for {perimeter.poly_Incid}\")\n",
    "#             continue\n",
    "\n",
    "#         held_pod['label'] = 'held_POD'\n",
    "#         print(\"2\")\n",
    "\n",
    "#         breached_pod = gpd.clip(inward_buffer_BREACH, test_pod_clipped)\n",
    "#         print(f\"Number of breached POD geometries: {len(breached_pod)}\")\n",
    "#         if breached_pod.empty:\n",
    "#             print(f\"No breached POD for {perimeter.poly_Incid}\")\n",
    "#             continue\n",
    "\n",
    "#         breached_pod_gdf = gpd.GeoDataFrame(geometry=breached_pod, crs=buffered_gdf.crs)\n",
    "#         breached_pod_gdf['label'] = 'breached_POD'\n",
    "#         print(\"3\")\n",
    "\n",
    "#         held_pod_points = generate_points_along_line(held_pod.geometry.iloc[0], interval_distance)\n",
    "#         print(\"3.5\")\n",
    "#         breached_pod_points = generate_points_along_line(breached_pod_gdf.geometry.iloc[0], interval_distance)\n",
    "#         print(\"4\")\n",
    "\n",
    "#         fire_perimeter_points = []\n",
    "#         for boundary in fire_perimeters.boundary:\n",
    "#             fire_perimeter_points.extend(generate_points_along_line(boundary, interval_distance))\n",
    "\n",
    "#         held_pod_df = gpd.GeoDataFrame(geometry=held_pod_points)\n",
    "#         breached_pod_df = gpd.GeoDataFrame(geometry=breached_pod_points)\n",
    "#         fire_perimeter_df = gpd.GeoDataFrame(geometry=fire_perimeter_points)\n",
    "\n",
    "#         merged_df = pd.concat([held_pod_df, breached_pod_df, fire_perimeter_df])\n",
    "\n",
    "#         held_labels = [1] * len(held_pod_points)\n",
    "#         breached_labels = [0] * len(breached_pod_points)\n",
    "#         fire_labels = [2] * len(fire_perimeter_points)\n",
    "#         labels = held_labels + breached_labels + fire_labels\n",
    "#         print(\"5\")\n",
    "\n",
    "#         data = {\n",
    "#             'labels': labels,\n",
    "#             'merged_geo_series_x': merged_df.geometry.x,\n",
    "#             'merged_geo_series_y': merged_df.geometry.y\n",
    "#         }\n",
    "\n",
    "#         df = pd.DataFrame(data)\n",
    "#         df['labels'] = df['labels'].astype(np.int16)\n",
    "#         df['merged_geo_series_x'] = df['merged_geo_series_x'].astype(np.float32)\n",
    "#         df['merged_geo_series_y'] = df['merged_geo_series_y'].astype(np.float32)\n",
    "\n",
    "#         held_and_breached_points = pd.concat([df[df['labels'] == 1], df[df['labels'] == 0]])\n",
    "#         held_and_breached_gdf = gpd.GeoDataFrame(\n",
    "#             held_and_breached_points,\n",
    "#             geometry=gpd.points_from_xy(held_and_breached_points.merged_geo_series_x, held_and_breached_points.merged_geo_series_y)\n",
    "#         ).set_crs(utm_crs)\n",
    "\n",
    "#         held_and_breached_gdf = gpd.sjoin_nearest(held_and_breached_gdf, test_pod_clipped[['geometry', 'FirelineCa']], how='left', distance_col='distance')\n",
    "\n",
    "#         print(f\"Centroid coordinates: Latitude = {centroid_lat}, Longitude = {centroid_lon}\")\n",
    "#         print(held_and_breached_gdf.head())\n",
    "\n",
    "#         unique_counts = held_and_breached_gdf['labels'].value_counts()\n",
    "#         print(unique_counts)\n",
    "\n",
    "#         held_and_breached_gdf['CatLabels'] = held_and_breached_gdf['FirelineCa'].astype(str) + held_and_breached_gdf['labels'].astype(str)\n",
    "#         unique_counts = held_and_breached_gdf['CatLabels'].value_counts()\n",
    "#         print(unique_counts)\n",
    "\n",
    "#         # Add the BreachHeld column\n",
    "#         #held_and_breached_gdf['BreachHeld'] = np.where(held_and_breached_gdf['labels'] == 1, 1, 0)\n",
    "\n",
    "#         # Transform to WGS 84 before saving\n",
    "#         held_and_breached_gdf_wgs84 = held_and_breached_gdf.to_crs(epsg=4326)\n",
    "\n",
    "#         # Generate the filename from the poly_Incid attribute\n",
    "#         poly_incid = perimeter.poly_Incid.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "#         output_shapefile_path = os.path.join(OutputFolder, f\"{poly_incid}_{interval_distance}m.shp\")\n",
    "\n",
    "#         # Save the GeoDataFrame to a shapefile\n",
    "#         held_and_breached_gdf_wgs84.to_file(output_shapefile_path, driver='ESRI Shapefile')\n",
    "#         print(f'GeoDataFrame saved to {output_shapefile_path}')\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing perimeter {perimeter.poly_Incid}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffc74a-c516-447f-b2f6-12c519cae51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from shapely.geometry import Polygon, LineString\n",
    "import utm\n",
    "\n",
    "# Constants\n",
    "interval_distance = 90\n",
    "buff = 350\n",
    "threshold_distance = 350\n",
    "\n",
    "# Paths to shapefiles\n",
    "shapefile_path = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\WFIGFirePerimeters2021\\Perimeters.shp\"\n",
    "OutputFolder = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\FLE_SamplePoints_1\\\\\"\n",
    "POD_shapefile_path = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\FL_2021.shp\"\n",
    "\n",
    "# Function to generate points along a line at a specified interval\n",
    "def generate_points_along_line(line, interval_distance):\n",
    "    total_length = line.length\n",
    "    num_points = int(total_length // interval_distance) + 1\n",
    "    return [line.interpolate(i * interval_distance) for i in range(num_points)]\n",
    "\n",
    "# Read the fire perimeter shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "filtered_gdf = gdf.sort_values(by='poly_Acres', ascending=False)  # Sort by polygon area (descending)\n",
    "\n",
    "# Read and filter the POD shapefile for required fireline categories\n",
    "test_pod = gpd.read_file(POD_shapefile_path)\n",
    "test_pod = test_pod[test_pod['FirelineCa'].isin([1, 2, 3]) & (test_pod['FirelineEn'] != 'Not Engaged')]\n",
    "\n",
    "# Main processing loop for each perimeter\n",
    "for i, perimeter in enumerate(filtered_gdf.itertuples()):\n",
    "    try:\n",
    "        fire_perimeter_gdf = gpd.GeoDataFrame([perimeter], geometry='geometry')\n",
    "\n",
    "        # Set CRS if missing\n",
    "        if fire_perimeter_gdf.crs is None:\n",
    "            fire_perimeter_gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "        # Convert perimeter coordinates to UTM\n",
    "        lon, lat = fire_perimeter_gdf.geometry.iloc[0].centroid.coords[0]\n",
    "        utm_zone = utm.from_latlon(lat, lon)[2]\n",
    "        utm_crs = f'EPSG:326{utm_zone}'\n",
    "        fire_perimeter_gdf = fire_perimeter_gdf.to_crs(utm_crs)\n",
    "\n",
    "        # Buffering operations for held and breached PODs\n",
    "        outward_buffer = fire_perimeter_gdf.geometry.buffer(buff)\n",
    "        inward_buffer = fire_perimeter_gdf.geometry.buffer(-buff)\n",
    "        inward_buffer_breach = fire_perimeter_gdf.geometry.buffer(-buff - 300)\n",
    "        combined_buffers = outward_buffer.difference(inward_buffer)\n",
    "        buffered_gdf = gpd.GeoDataFrame(geometry=combined_buffers, crs=fire_perimeter_gdf.crs)\n",
    "\n",
    "        # Clip PODs using buffered fire perimeters\n",
    "        test_pod_transformed = test_pod.to_crs(buffered_gdf.crs)\n",
    "        buffered_fire_perimeter = fire_perimeter_gdf.buffer(buff)\n",
    "        test_pod_clipped = gpd.clip(test_pod_transformed, buffered_fire_perimeter)\n",
    "\n",
    "        # Identify held and breached PODs\n",
    "        held_pod = gpd.clip(buffered_gdf, test_pod_clipped)\n",
    "        if held_pod.empty:\n",
    "            print(f\"No held POD for {perimeter.poly_Incid}\")\n",
    "            continue\n",
    "\n",
    "        breached_pod = gpd.clip(inward_buffer_breach, test_pod_clipped)\n",
    "        if breached_pod.empty:\n",
    "            print(f\"No breached POD for {perimeter.poly_Incid}\")\n",
    "            continue\n",
    "\n",
    "        # Generate points along held and breached PODs\n",
    "        held_pod_points = generate_points_along_line(held_pod.geometry.iloc[0], interval_distance)\n",
    "        breached_pod_points = generate_points_along_line(breached_pod.geometry.iloc[0], interval_distance)\n",
    "\n",
    "        # Generate points along fire perimeter boundaries\n",
    "        fire_perimeter_points = []\n",
    "        for boundary in fire_perimeter_gdf.boundary:\n",
    "            fire_perimeter_points.extend(generate_points_along_line(boundary, interval_distance))\n",
    "\n",
    "        # Combine held, breached, and fire perimeter points into a GeoDataFrame\n",
    "        held_labels = [1] * len(held_pod_points)\n",
    "        breached_labels = [0] * len(breached_pod_points)\n",
    "        fire_labels = [2] * len(fire_perimeter_points)\n",
    "        all_labels = held_labels + breached_labels + fire_labels\n",
    "\n",
    "        merged_df = pd.concat([\n",
    "            gpd.GeoDataFrame(geometry=held_pod_points),\n",
    "            gpd.GeoDataFrame(geometry=breached_pod_points),\n",
    "            gpd.GeoDataFrame(geometry=fire_perimeter_points)\n",
    "        ])\n",
    "\n",
    "        # Create DataFrame with labels\n",
    "        data = {\n",
    "            'labels': all_labels,\n",
    "            'x': merged_df.geometry.x,\n",
    "            'y': merged_df.geometry.y\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Create GeoDataFrame with CRS\n",
    "        held_and_breached_gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(df['x'], df['y']),\n",
    "            crs=utm_crs\n",
    "        )\n",
    "\n",
    "        # Spatial join with nearest FirelineCa from clipped PODs\n",
    "        held_and_breached_gdf = gpd.sjoin_nearest(held_and_breached_gdf, test_pod_clipped[['geometry', 'FirelineCa']], how='left', distance_col='distance')\n",
    "\n",
    "        # Print and analyze label counts\n",
    "        print(held_and_breached_gdf['labels'].value_counts())\n",
    "\n",
    "        # Generate categorical labels\n",
    "        held_and_breached_gdf['CatLabels'] = held_and_breached_gdf['FirelineCa'].astype(str) + held_and_breached_gdf['labels'].astype(str)\n",
    "        print(held_and_breached_gdf['CatLabels'].value_counts())\n",
    "\n",
    "        # Transform to WGS 84 before saving\n",
    "        held_and_breached_gdf_wgs84 = held_and_breached_gdf.to_crs(epsg=4326)\n",
    "\n",
    "        # Save the GeoDataFrame to a shapefile\n",
    "        poly_incid = perimeter.poly_Incid.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        output_shapefile_path = os.path.join(OutputFolder, f\"{poly_incid}_{interval_distance}m.shp\")\n",
    "        held_and_breached_gdf_wgs84.to_file(output_shapefile_path, driver='ESRI Shapefile')\n",
    "\n",
    "        print(f'GeoDataFrame saved to {output_shapefile_path}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing perimeter {perimeter.poly_Incid}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b0be9-d69b-49ad-b0b3-55dab326ff7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27639a45-32fc-45ea-aec5-c89e77c5a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This combines all individual shapefiles into one to make one complete dataset\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the shapefiles\n",
    "directory = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\FLE_SamplePoints_1\"\n",
    "\n",
    "# Find all the shapefiles in the directory\n",
    "shapefiles = glob.glob(os.path.join(directory, \"*.shp\"))\n",
    "\n",
    "# Read and combine all shapefiles into one GeoDataFrame\n",
    "gdf_list = [gpd.read_file(shp) for shp in shapefiles]\n",
    "combined_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True))\n",
    "\n",
    "# Save the combined GeoDataFrame as a new shapefile in the same directory\n",
    "output_path = os.path.join(directory, \"combined.shp\")\n",
    "combined_gdf.to_file(output_path)\n",
    "\n",
    "print(f\"Combined shapefile saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aebff8-e4f0-496e-a72b-f21307e84ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2cd35d-c6d9-4c86-984c-f6dfab792d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Progress bar library\n",
    "from rasterio.vrt import WarpedVRT\n",
    "from rasterio import sample\n",
    "\n",
    "def extract_multi_values_to_points(points_shapefile, raster_files, reference_raster, output_csv=None):\n",
    "    # Step 1: Load the point shapefile using GeoPandas\n",
    "    points_gdf = gpd.read_file(points_shapefile)\n",
    "\n",
    "    # Step 2: Load the reference raster to get the correct CRS\n",
    "    with rasterio.open(reference_raster) as ref_src:\n",
    "        target_crs = ref_src.crs\n",
    "\n",
    "    # Step 3: Reproject point geometries if necessary\n",
    "    if points_gdf.crs != target_crs:\n",
    "        points_gdf = points_gdf.to_crs(target_crs)\n",
    "    \n",
    "    # Initialize a DataFrame to store the extracted values, including original attributes\n",
    "    extracted_values = points_gdf.copy()\n",
    "\n",
    "    # Convert point geometries to a list of coordinate tuples (x, y)\n",
    "    point_coords = [(geom.x, geom.y) for geom in points_gdf.geometry]\n",
    "\n",
    "    # Step 4: Loop through each raster file and extract the values at the point locations\n",
    "    for raster_path in tqdm(raster_files, desc=\"Processing Rasters\"):\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            # Handle CRS reprojection if necessary\n",
    "            if src.crs != target_crs:\n",
    "                vrt_options = {\n",
    "                    'crs': target_crs,\n",
    "                    'resampling': rasterio.enums.Resampling.nearest\n",
    "                }\n",
    "                with WarpedVRT(src, **vrt_options) as vrt:\n",
    "                    values = [val[0] for val in sample.sample_gen(vrt, point_coords)]\n",
    "            else:\n",
    "                # Extract the raster values at all point locations in one go\n",
    "                values = [val[0] for val in sample.sample_gen(src, point_coords)]\n",
    "        \n",
    "        # Get the raster filename to use as a column name\n",
    "        column_name = raster_path.split(\"\\\\\")[-1].replace('.tif', '')\n",
    "        extracted_values[column_name] = values\n",
    "    \n",
    "    # Step 5: Optionally save to CSV\n",
    "    if output_csv:\n",
    "        extracted_values.to_csv(output_csv, index=False)\n",
    "        print(f\"Extracted values saved to: {output_csv}\")\n",
    "    \n",
    "    return extracted_values\n",
    "\n",
    "# Example usage:\n",
    "points_shapefile = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\FLE_SamplePoints_1\\combined.shp\"\n",
    "reference_raster = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\HFTCOMPELTEDATASET\\US_140_PRS\\US_140_PRS\\Tif\\us_140prs.tif\"\n",
    "\n",
    "raster_files = [\n",
    "    r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\SDI\\SDI_2021_CONUS\\2021_90th_uphill_x100_CONUS\\CONUS_2021_90_SDI.tif\",\n",
    "    r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\LF\\LF2020_Asp_220_CONUS\\Tif\\LC20_Asp_220.tif\",\n",
    "    r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\LF\\LF2020_SlpD_220_CONUS\\Tif\\LC20_SlpD_220.tif\"\n",
    "]\n",
    "\n",
    "output_csv = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\FLE_SamplePoints_1\\extracted_valuesfull_FLE.csv\"\n",
    "\n",
    "# Call the function\n",
    "extracted_df = extract_multi_values_to_points(points_shapefile, raster_files, reference_raster, output_csv)\n",
    "print(\"done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f5691eb-3381-472d-8ce2-6ce309eaf913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique CatLabels: ['1.01' '1.00']\n",
      "\n",
      "Number of occurrences for each CatLabel:\n",
      "CatLabels\n",
      "1.01    45651\n",
      "1.00    26889\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New number of occurrences after subsampling:\n",
      "CatLabels\n",
      "1.01    26889\n",
      "1.00    26889\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Define the directory and output path\n",
    "output_csv = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\FLE_SamplePoints_1\\extracted_valuesfull_FLE.csv\"\n",
    "\n",
    "# Read the CSV into a GeoDataFrame\n",
    "df_filtered = gpd.read_file(output_csv)\n",
    "\n",
    "# Filter the DataFrame based on specific 'CatLabels'\n",
    "df_filtered = df_filtered[df_filtered['CatLabels'].isin(['1.01', '1.00'])]\n",
    "\n",
    "# Print the unique labels\n",
    "print(\"Unique CatLabels:\", df_filtered['CatLabels'].unique())\n",
    "\n",
    "# Print the number of occurrences for each label\n",
    "label_counts = df_filtered['CatLabels'].value_counts()\n",
    "print(\"\\nNumber of occurrences for each CatLabel:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Find the label with fewer occurrences\n",
    "min_label = label_counts.idxmin()\n",
    "min_count = label_counts.min()\n",
    "\n",
    "# Subsample the label with more occurrences to match the label with fewer occurrences\n",
    "df_more = df_filtered[df_filtered['CatLabels'] != min_label]\n",
    "df_less = df_filtered[df_filtered['CatLabels'] == min_label]\n",
    "\n",
    "# Randomly subsample the one with more occurrences to match the count of the lesser label\n",
    "df_more_subsampled = df_more.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the subsampled dataframes\n",
    "df_filtered = pd.concat([df_more_subsampled, df_less])\n",
    "\n",
    "# Print the new distribution\n",
    "new_label_counts = df_filtered['CatLabels'].value_counts()\n",
    "print(\"\\nNew number of occurrences after subsampling:\")\n",
    "print(new_label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7ebb4-98c3-41ef-a6b9-bf9292a298b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9c863a-119e-4644-a82c-3eeeed77c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names: ['1.00' '1.01']\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\fire1\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m589/589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5545 - auc: 0.5686 - loss: 0.6867 - val_accuracy: 0.5739 - val_auc: 0.6232 - val_loss: 0.6725\n",
      "Epoch 2/5\n",
      "\u001b[1m589/589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5882 - auc: 0.6354 - loss: 0.6684 - val_accuracy: 0.6008 - val_auc: 0.6569 - val_loss: 0.6593\n",
      "Epoch 3/5\n",
      "\u001b[1m589/589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6029 - auc: 0.6515 - loss: 0.6601 - val_accuracy: 0.6134 - val_auc: 0.6608 - val_loss: 0.6548\n",
      "Epoch 4/5\n",
      "\u001b[1m589/589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6141 - auc: 0.6603 - loss: 0.6549 - val_accuracy: 0.6312 - val_auc: 0.6676 - val_loss: 0.6522\n",
      "Epoch 5/5\n",
      "\u001b[1m589/589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6237 - auc: 0.6666 - loss: 0.6520 - val_accuracy: 0.6264 - val_auc: 0.6680 - val_loss: 0.6520\n",
      "\u001b[1m505/505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6238 - auc: 0.6672 - loss: 0.6519\n",
      "Test Accuracy: 0.6263790726661682\n",
      "Test AUC: 0.6680361032485962\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "# Assuming df_filtered is already loaded and contains the relevant columns and 'Treatment_Label'\n",
    "\n",
    "# Define the features \n",
    "X = df_filtered[['CONUS_2021_90_SDI', 'LC20_Asp_220', 'LC20_SlpD_220']]\n",
    "\n",
    "# Define the labels\n",
    "y = df_filtered['CatLabels']\n",
    "\n",
    "# One-hot encode the target\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = one_hot_encoder.fit_transform(y.values.reshape(-1, 1))\n",
    "# Print class names\n",
    "class_names = one_hot_encoder.categories_[0]\n",
    "print(f\"Class Names: {class_names}\")\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# MinMax scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(2, activation='softmax')) # two output classes \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy', AUC(name='auc')])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5, batch_size=64, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy, auc = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(f'Test AUC: {auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4d227-e213-479b-982f-d6e80ffc0475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15849214-4fa0-49c6-8b0a-515c94e02c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rasters: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted values saved to: C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\HFTCOMPELTEDATASET\\extracted_model_values_no_filter_1.csv\n",
      "\u001b[1m3820/3820\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step\n",
      "Probability raster for class 1 saved to C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\FLE_SamplePoints_1\\predicted_prob_no_filterCLASS1_1_class_1.tif\n",
      "Probability raster for class 2 saved to C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\FLE_SamplePoints_1\\predicted_prob_no_filterCLASS1_1_class_2.tif\n",
      "Processing and rasterization completed for all raster values.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rasterio.vrt import WarpedVRT\n",
    "from joblib import Parallel, delayed\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "# List of raster values (you can modify or extend this as needed)\n",
    "raster_values = [1]\n",
    "\n",
    "# Function to process a single raster and extract values\n",
    "def process_raster(raster_path, point_coords, target_crs):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        if src.crs != target_crs:\n",
    "            vrt_options = {'crs': target_crs, 'resampling': rasterio.enums.Resampling.nearest}\n",
    "            with WarpedVRT(src, **vrt_options) as vrt:\n",
    "                values = np.array(list(vrt.sample(point_coords))).flatten()\n",
    "        else:\n",
    "            values = np.array(list(src.sample(point_coords))).flatten()\n",
    "    return values\n",
    "\n",
    "# Main function to extract multiple raster values for points and save to CSV\n",
    "def extract_multi_values_to_points(points_shapefile, raster_files, reference_raster, output_csv, n_jobs=4):\n",
    "    # Load the point shapefile using GeoPandas\n",
    "    points_gdf = gpd.read_file(points_shapefile)\n",
    "\n",
    "    # Load the reference raster to get the correct CRS\n",
    "    with rasterio.open(reference_raster) as ref_src:\n",
    "        target_crs = ref_src.crs\n",
    "\n",
    "    # Reproject point geometries if necessary\n",
    "    if points_gdf.crs != target_crs:\n",
    "        points_gdf = points_gdf.to_crs(target_crs)\n",
    "\n",
    "    # Initialize an empty DataFrame to store the extracted values\n",
    "    extracted_values = pd.DataFrame()\n",
    "    extracted_values['geometry'] = points_gdf['geometry']\n",
    "\n",
    "    # Convert point geometries to a list of coordinate tuples (x, y)\n",
    "    point_coords = [(geom.x, geom.y) for geom in points_gdf.geometry]\n",
    "\n",
    "    # Parallelize raster extraction using joblib\n",
    "    extracted_values_list = Parallel(n_jobs=n_jobs)(delayed(process_raster)(raster_path, point_coords, target_crs) for raster_path in tqdm(raster_files, desc=\"Processing Rasters\"))\n",
    "\n",
    "    # Convert the list of arrays to DataFrame columns\n",
    "    for idx, values in enumerate(extracted_values_list):\n",
    "        column_name = raster_files[idx].split(\"\\\\\")[-1].replace('.tif', '')\n",
    "        extracted_values[column_name] = values\n",
    "\n",
    "    # Save to CSV\n",
    "    extracted_values.to_csv(output_csv, index=False)\n",
    "    print(f\"Extracted values saved to: {output_csv}\")\n",
    "\n",
    "    return extracted_values\n",
    "\n",
    "# Rasterize the probability predictions for each raster value\n",
    "def rasterize_probabilities(extracted_df, softmax_predictions, reference_raster, output_prefix):\n",
    "    with rasterio.open(reference_raster) as src:\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "\n",
    "    nodata_value = -9999  # Set NoData value\n",
    "\n",
    "    for i in range(softmax_predictions.shape[1]):  # Loop over each class\n",
    "        class_predictions = softmax_predictions[:, i]\n",
    "\n",
    "        # Rasterize the point predictions within the raster grid\n",
    "        prob_raster = rasterize(\n",
    "            ((geom, value) for geom, value in zip(extracted_df['geometry'], class_predictions)),\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=nodata_value,  # Set NoData value as fill\n",
    "            all_touched=True,\n",
    "            dtype='float32'\n",
    "        )\n",
    "        \n",
    "        # Mask NoData cells\n",
    "        prob_raster = np.ma.masked_equal(prob_raster, nodata_value)\n",
    "\n",
    "        output_tif = f\"{output_prefix}_class_{i+1}.tif\"\n",
    "\n",
    "        with rasterio.open(\n",
    "            output_tif, 'w',\n",
    "            driver='GTiff',\n",
    "            height=height,\n",
    "            width=width,\n",
    "            count=1,\n",
    "            dtype='float32',\n",
    "            crs=crs,\n",
    "            transform=transform,\n",
    "            nodata=nodata_value  # Set NoData value in the output\n",
    "        ) as dst:\n",
    "            dst.write(prob_raster.filled(nodata_value), 1)  # Write masked array, filling NoData\n",
    "\n",
    "        print(f\"Probability raster for class {i+1} saved to {output_tif}\")\n",
    "\n",
    "# Loop through each raster value and extract data\n",
    "for i in range(len(raster_values)):\n",
    "    points_shapefile = f\"C:\\\\Users\\\\magst\\\\OneDrive\\\\Pictures\\\\Desktop\\\\HFTCOMPELTEDATASET\\\\Selected_NF_{raster_values[i]}_points.shp\"\n",
    "    reference_raster = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\HFTCOMPELTEDATASET\\NatForest_CO_Name.tif\"\n",
    "    \n",
    "    # List of raster files for extraction\n",
    "    raster_files = [\n",
    "        r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\SDI\\SDI_2021_CONUS\\2021_90th_uphill_x100_CONUS\\CONUS_2021_90_SDI.tif\",\n",
    "        r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\LF\\LF2020_Asp_220_CONUS\\Tif\\LC20_Asp_220.tif\",\n",
    "        r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\CONUSDATA\\LF\\LF2020_SlpD_220_CONUS\\Tif\\LC20_SlpD_220.tif\"\n",
    "    ]\n",
    "    \n",
    "    # Output CSV path for each iteration\n",
    "    output_csv = f\"C:\\\\Users\\\\magst\\\\OneDrive\\\\Pictures\\\\Desktop\\\\HFTCOMPELTEDATASET\\\\extracted_model_values_no_filter_{raster_values[i]}.csv\"\n",
    "    \n",
    "    # Extract raster values for current points shapefile\n",
    "    extracted_df = extract_multi_values_to_points(points_shapefile, raster_files, reference_raster, output_csv, n_jobs=4)\n",
    "    \n",
    "    # Step 2: Prepare features for prediction if the DataFrame is not empty\n",
    "    if not extracted_df.empty:\n",
    "        features = extracted_df.drop(columns=['geometry'])\n",
    "        scaled_features = scaler.transform(features)  # Assuming 'scaler' is pre-fitted\n",
    "    \n",
    "        # Step 3: Predict using the pre-trained model\n",
    "        softmax_predictions = model.predict(scaled_features)\n",
    "    \n",
    "        # Step 4: Rasterize and save the probability rasters\n",
    "        output_prefix = f\"C:\\\\Users\\\\magst\\\\OneDrive\\\\Pictures\\\\Desktop\\\\FLE_SamplePoints_1\\\\predicted_prob_no_filterCLASS1_{raster_values[i]}\"\n",
    "        rasterize_probabilities(extracted_df, softmax_predictions, reference_raster, output_prefix)\n",
    "\n",
    "print(\"Processing and rasterization completed for all raster values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190f556-fa86-4ad8-9d61-6648e871161d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
