{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700b08e-f904-45fe-afea-2af5a08a405b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS UNZIPS\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "def get_links(url):\n",
    "    response = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    return links\n",
    "\n",
    "def get_all_files(url):\n",
    "    response = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    out = []\n",
    "    for link in links[5:]:\n",
    "        href = link.get('href')\n",
    "        subdir_url = url + href\n",
    "        if subdir_url.endswith('/'):  # Check if the link is a directory\n",
    "            out.append(get_all_files(subdir_url))\n",
    "        else:  # if not, add download link to outlist\n",
    "            out.append(subdir_url)\n",
    "    return out\n",
    "\n",
    "def flatten_list(lst):\n",
    "    flattened = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            flattened.extend(flatten_list(item))\n",
    "        else:\n",
    "            flattened.append(item)\n",
    "    return flattened\n",
    "\n",
    "url = 'https://ftp.wildfire.gov/public/incident_specific_data/rocky_mtn/'\n",
    "region = 'rocky_mtn'\n",
    "fyeardirs = ['2020/']  # Find all the links on the page\n",
    "\n",
    "allout = []\n",
    "links = get_links(url)\n",
    "for link in links[5:]:\n",
    "    href = link.get('href')\n",
    "    if href not in fyeardirs:\n",
    "        continue\n",
    "    subdir_url = url + href\n",
    "    sublinks = get_links(subdir_url)\n",
    "    for sl in sublinks[5:]:\n",
    "        href = sl.get('href')\n",
    "        su = subdir_url + href\n",
    "        # force IR dir\n",
    "        irdir = su + 'IR/'  # THIS IS NOT CONSISTENT AMONG REGIONS AND ACROSS YEARS (so please check it)\n",
    "        allout.append(get_all_files(irdir))\n",
    "\n",
    "allout = flatten_list(allout)\n",
    "print(len(allout))\n",
    "\n",
    "# Directory to save the downloaded files\n",
    "local_directory = 'C:\\\\Users\\\\magst\\\\Desktop\\\\NIROPS_Data'\n",
    "\n",
    "# Function to download a file from a URL and maintain the directory structure\n",
    "def download_file(url, base_url, local_base_directory):\n",
    "    try:\n",
    "        # Extract relative path\n",
    "        relative_path = url.replace(base_url, '')\n",
    "        local_path = os.path.join(local_base_directory, relative_path)\n",
    "        \n",
    "        # Create necessary directories\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        \n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024 # 1 Kibibyte\n",
    "        with open(local_path, 'wb') as file, tqdm(\n",
    "            total=total_size_in_bytes, unit='iB', unit_scale=True, desc=relative_path, leave=False) as progress_bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Unzip if the file is a ZIP file\n",
    "        if local_path.endswith('.zip'):\n",
    "            with zipfile.ZipFile(local_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(os.path.dirname(local_path))\n",
    "            os.remove(local_path)  # Remove the ZIP file after extraction\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}. Reason: {e}\")\n",
    "\n",
    "# Using ThreadPoolExecutor to download files concurrently\n",
    "def download_files(urls, base_url, local_base_directory):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(download_file, url, base_url, local_base_directory) for url in urls]\n",
    "        for future in futures:\n",
    "            try:\n",
    "                future.result()  # Ensure all futures are completed\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred during downloading: {e}\")\n",
    "\n",
    "# Download all files concurrently\n",
    "download_files(allout, url, local_directory)\n",
    "print(\"Download completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
